ğŸ“Š Stock Market Daily Data Pipeline

ğŸ“Œ Project Overview

This project implements an end-to-end data engineering pipeline to ingest, process, model, and analyze daily stock market data using the Alpha Vantage API and Snowflake as the cloud data platform.

The pipeline handles semi-structured JSON data, transforms it into analytics-ready star schema tables, applies data quality checks, and automates processing using Snowflake Tasks.


ğŸ§­ Project Track & Platform
	â€¢	Track: Track D â€” Stock Market Daily Pipeline
	â€¢	Cloud Platform: Snowflake (AWS)
	â€¢	Programming Language: Python & SQL


ğŸ”— Data Source
	â€¢	Alpha Vantage API
https://www.alphavantage.co/documentation/

The project uses the TIME_SERIES_DAILY endpoint to retrieve:
	â€¢	Open price
	â€¢	High price
	â€¢	Low price
	â€¢	Close price
	â€¢	Volume


ğŸ—ï¸ Architecture Overview
Alpha Vantage API
        â†“
Python Ingestion Script
        â†“
Landing Zone (Raw JSON)
        â†“
Snowflake Internal Stage
        â†“
RAW Schema (VARIANT JSON)
        â†“
Flattened Relational View
        â†“
STAR Schema (Fact & Dimensions)
        â†“
Automated Refresh (Snowflake Task)

## ğŸ“‚ Repository Structure

```text
stock-market-data-pipeline/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ ingest.py
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ create_tables.sql
â”‚   â”œâ”€â”€ star_schema.sql
â”‚   â””â”€â”€ automation.sql
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ er_diagram.png
â”‚   â””â”€â”€ star_schema.png
â”œâ”€â”€ landing_zone/
â”‚   â””â”€â”€ daily_stock_AAPL_*.json
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt

âš™ï¸ Data Ingestion (Week 5)
	â€¢	Data is fetched from Alpha Vantage using a Python script (ingest.py)
	â€¢	API credentials are securely handled using environment variables
	â€¢	Raw API responses are stored as JSON files in the landing zone

Secure API Key Handling
export ALPHAVANTAGE_API_KEY=your_api_key
ğŸ§± Data Modeling (Week 6)
	â€¢	A 3NF (normalized) schema was designed to eliminate redundancy
	â€¢	An ER diagram documents entities and relationships
	â€¢	Deduplication is handled using:
	â€¢	Stock symbol
	â€¢	Trade date as a natural key

â­ Star Schema Design (Week 7)

To support analytical workloads, the data is transformed into a star schema:

Dimension Tables
	â€¢	dim_stock â€” unique stock symbols
	â€¢	dim_date â€” calendar attributes

Fact Table
	â€¢	fact_daily_stock_price â€” daily stock prices and volume

This structure enables:
	â€¢	Fast aggregations
	â€¢	Time-series analysis
	â€¢	BI and reporting use cases


ğŸ” Handling Semi-Structured Data
	â€¢	Raw JSON is stored using Snowflakeâ€™s VARIANT data type
	â€¢	Nested time-series data is transformed using:
	â€¢	LATERAL FLATTEN()
	â€¢	Each trading day becomes a relational row


ğŸ›¡ï¸ Governance & Data Quality (Week 8)

Data Quality Rules
	â€¢	No negative prices
	â€¢	No negative volume
	â€¢	Mandatory stock symbol and trade date

Validation is enforced using:
	â€¢	Filtered inserts
	â€¢	Validation queries at the STAR layer

Example:
SELECT *
FROM STAR.fact_daily_stock_price
WHERE open_price < 0
   OR close_price < 0
   OR volume < 0;

ğŸ” Security Best Practices
	â€¢	API keys are stored in environment variables
	â€¢	No secrets are committed to GitHub
	â€¢	.env files are excluded from version control


â±ï¸ Automation
	â€¢	Snowflake Tasks are used to automate daily refresh of star schema tables
	â€¢	The pipeline runs on a scheduled basis without manual intervention

ğŸ¥ Final Demonstration

A 5-minute demo video showcases:
	â€¢	Data ingestion
	â€¢	Snowflake RAW â†’ STAR transformation
	â€¢	Automation using Snowflake Tasks
	â€¢	Final analytical tables


ğŸš§ Challenges Faced
	â€¢	Handling deeply nested JSON structures
	â€¢	Managing Snowflake schema context
	â€¢	Understanding VARIANT and FLATTEN usage
	â€¢	Working within API rate limits


âœ… Final Outcome

The project delivers:
	â€¢	A fully automated, governed data pipeline
	â€¢	Analytics-ready star schema tables
	â€¢	Secure, scalable cloud implementation
	â€¢	Complete documentation and version control


ğŸ‘¤ Author

Siddharth Shetty
GitHub: https://github.com/siddharth1956
